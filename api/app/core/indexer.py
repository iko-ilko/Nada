"""
ë¬¸ì„œ ì¸ë±ì‹± ëª¨ë“ˆ
ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.

í¬í•¨ëœ í´ë˜ìŠ¤:
- DocumentLoader: PDF/TXT íŒŒì¼ ë¡œë“œ
- TextChunker: ë¬¸ì„œ ì²­í‚¹
- EmbeddingManager: ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬
- VectorStoreManager: ë²¡í„° DB ê´€ë¦¬
- DocumentIndexer: ì „ì²´ ì¸ë±ì‹± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""
import os
import logging
import time
from pathlib import Path

logger = logging.getLogger(__name__)

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

from app.core.config import Config


class DocumentLoader:
    """ë¬¸ì„œ ë¡œë”: PDFì™€ TXT íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""

    def __init__(self, folder_path=None):
        self.folder_path = folder_path or Config.DATA_DIR

    def load_documents(self):
        """í´ë” ì•ˆì˜ ëª¨ë“  PDFì™€ TXT íŒŒì¼ì„ ë¡œë“œ

        PDF íŒŒì¼ì€ í˜ì´ì§€ë“¤ì„ ë³‘í•©í•˜ì—¬ í•œ ë¬¸ì„œë¡œ ë§Œë“­ë‹ˆë‹¤.
        (ì²­í‚¹ ì „ì— í˜ì´ì§€ë¥¼ ë‚˜ëˆ„ë©´ ì˜ë¯¸ ìˆëŠ” ì²­í‚¹ì´ ë¶ˆê°€ëŠ¥)
        """
        documents = []

        if not os.path.exists(self.folder_path):
            logger.error(f"âŒ í´ë” ì—†ìŒ: {self.folder_path}")
            return documents

        pdf_files = list(Path(self.folder_path).glob("*.pdf"))
        txt_files = list(Path(self.folder_path).glob("*.txt"))

        logger.info(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì¤‘... (PDF: {len(pdf_files)}, TXT: {len(txt_files)})")

        if len(pdf_files) == 0 and len(txt_files) == 0:
            logger.warning("âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return documents

        # PDF ë¡œë“œ: í˜ì´ì§€ë“¤ì„ ë³‘í•©í•˜ì—¬ í•œ ë¬¸ì„œë¡œ
        for pdf_file in pdf_files:
            try:
                loader = PyPDFLoader(str(pdf_file))
                pages = loader.load()

                if pages is None:
                    logger.warning(f"   âš ï¸  {pdf_file.name}: loader returned None")
                    continue

                total_pages = len(pages)
                valid_pages = [p for p in pages if getattr(p, "page_content", "") and p.page_content.strip()]
                valid_count = len(valid_pages)
                chars = sum(len(p.page_content) for p in valid_pages) if valid_pages else 0
                logger.info(f"   [{pdf_file.name}] pages_loaded={total_pages}, valid_pages={valid_count}, chars={chars}")

                if not valid_pages:
                    logger.warning(f"   âš ï¸  {pdf_file.name}: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ (OCR í•„ìš” ê°€ëŠ¥ì„±)")
                    continue

                merged_content = "\n\n".join([p.page_content for p in valid_pages])

                merged_doc = Document(
                    page_content=merged_content,
                    metadata={
                        "source": pdf_file.name,
                        "type": "pdf",
                        "total_pages": valid_count,
                        "start_page": 0,
                        "end_page": valid_count - 1,
                    },
                )

                documents.append(merged_doc)
            except Exception as e:
                logger.error(f"   âŒ {pdf_file.name}: {e}")

        # TXT ë¡œë“œ
        for txt_file in txt_files:
            try:
                loader = TextLoader(str(txt_file), encoding="utf-8")
                docs = loader.load()

                for doc in docs:
                    wrapped = Document(page_content=doc.page_content, metadata={
                        "source": txt_file.name,
                        "type": "txt",
                    })
                    documents.append(wrapped)
            except Exception as e:
                logger.error(f"   âŒ {txt_file.name}: {e}")

        logger.info(f"   âœ… {len(pdf_files) + len(txt_files)}ê°œ íŒŒì¼ì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ")
        return documents


class TextChunker:
    """ë¬¸ì„œ ì²­í‚¹: ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""

    def __init__(self, chunk_size=None, chunk_overlap=None):
        self.chunk_size = chunk_size or Config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or Config.CHUNK_OVERLAP

    def chunk_documents(self, documents):
        """ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• """
        logger.info(f"âœ‚ï¸  ì²­í‚¹ ì¤‘... (í¬ê¸°: {self.chunk_size}, ì˜¤ë²„ë©: {self.chunk_overlap})")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ". ", " "],
            keep_separator=False,
        )

        chunks = splitter.split_documents(documents)

        min_length = Config.MIN_CHUNK_SIZE
        filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content.strip()) >= min_length]

        removed_count = len(chunks) - len(filtered_chunks)
        logger.info(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± â†’ {removed_count}ê°œ ì œê±° â†’ {len(filtered_chunks)}ê°œ ìµœì¢…")

        return filtered_chunks


class EmbeddingManager:
    """ì„ë² ë”© ê´€ë¦¬ì: ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""

    def __init__(self, model_name=None, device=None):
        self.model_name = model_name or Config.EMBEDDING_MODEL
        self.device = device or os.getenv("HF_EMBEDDING_DEVICE", "cpu")
        self.embeddings = None

    def get_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ ë¡œë“œì‹œë§Œ ë‹¤ìš´ë¡œë“œ)"""
        if self.embeddings is not None:
            return self.embeddings

        logger.info(f"ğŸ”¢ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... ({self.model_name}) device={self.device}")

        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.model_name,
                model_kwargs={"device": self.device},
                encode_kwargs={'normalize_embeddings': True}
            )
        except TypeError:
            self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)

        logger.info(f"   âœ… ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
        return self.embeddings


class VectorStoreManager:
    """ë²¡í„° DB ê´€ë¦¬ì: Chroma ë²¡í„° DBë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤."""

    def __init__(self, embeddings):
        self.embeddings = embeddings
        self.persist_dir = Config.CHROMA_DB_PATH
        self.collection_name = "papers"
        self.vectorstore = None

    def create_vectorstore(self, chunks):
        """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ê³  ë²¡í„° DBì— ì €ì¥"""
        logger.info(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì¤‘... ({len(chunks)}ê°œ ì²­í¬)")

        start_time = time.time()

        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_dir,
        )

        elapsed_time = time.time() - start_time
        logger.info(f"   âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ (ì†Œìš”: {elapsed_time:.2f}ì´ˆ)")
        return self.vectorstore

    def load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ"""
        if not os.path.exists(self.persist_dir):
            raise FileNotFoundError(f"ë²¡í„° DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.persist_dir}")

        logger.info(f"ğŸ“‚ ë²¡í„° DB ë¡œë“œ ì¤‘...")

        self.vectorstore = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings,
        )

        logger.info(f"   âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
        return self.vectorstore

    def get_retriever(self):
        """Retriever ë°˜í™˜ (ê²€ìƒ‰ìš©)"""
        if self.vectorstore is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": Config.TOP_K},
        )

    def get_bm25_retriever(self, documents):
        """BM25 Retriever ìƒì„±"""
        from langchain_community.retrievers import BM25Retriever
        logger.info(f"ğŸ” BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
        return BM25Retriever.from_documents(documents)


class DocumentIndexer:
    """ë¬¸ì„œ ì¸ë±ì‹± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"""

    def __init__(self):
        self.loader = DocumentLoader()
        self.chunker = TextChunker()
        self.embedding_manager = EmbeddingManager()
        self.db_manager = None

    def build_vectorstore(self):
        """
        ë²¡í„° DB ìƒì„±
        ë¬¸ì„œ ë¡œë“œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ë²¡í„° DB ì €ì¥

        Returns:
            VectorStoreManager: ìƒì„±ëœ ë²¡í„° DB ê´€ë¦¬ì
        """
        logger.info("ğŸ“‘ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")

        documents = self.loader.load_documents()
        if len(documents) == 0:
            logger.error("âŒ ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None

        chunks = self.chunker.chunk_documents(documents)

        embeddings = self.embedding_manager.get_embeddings()
        self.db_manager = VectorStoreManager(embeddings)
        self.db_manager.create_vectorstore(chunks)

        logger.info(f"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ")
        return self.db_manager

    def get_or_create_vectorstore(self):
        """
        ê¸°ì¡´ ë²¡í„° DBê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±

        Returns:
            VectorStoreManager: ë²¡í„° DB ê´€ë¦¬ì
        """
        embeddings = self.embedding_manager.get_embeddings()
        db_manager = VectorStoreManager(embeddings)

        if os.path.exists(Config.CHROMA_DB_PATH):
            logger.info(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„° DB ë°œê²¬")
            try:
                db_manager.load_vectorstore()
                logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
                self.db_manager = db_manager
                return db_manager
            except Exception as e:
                logger.warning(f"âš ï¸  ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.info(f"   ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")

        self.db_manager = self.build_vectorstore()
        return self.db_manager
