"""
ë¶„ì„ ì„œë¹„ìŠ¤
"""
import os
import logging
import hashlib
from pathlib import Path
from app.core.config import Config
from app.core.indexer import EmbeddingManager, VectorStoreManager
from app.core.llm import get_llm
from app.core.rag import build_analysis_chain
from app.core.vision import extract_json
from app.core.chain_logger import ChainLogger
from app.schemas.request import AnalysisRequest
from app.schemas.response import AnalysisResponse
from app.utils import cloudinary

logger = logging.getLogger(__name__)


def _merge_with_rrf(dense_docs, sparse_docs, k=60):
    """RRF (Reciprocal Rank Fusion)ë¡œ ë‘ ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ ë³‘í•©

    Returns:
        tuple: (merged_docs, rrf_scores_dict)
            - merged_docs: RRF ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ëœ Document ë¦¬ìŠ¤íŠ¸
            - rrf_scores_dict: {source_key: rrf_score} ë”•ì…”ë„ˆë¦¬
    """
    scores = {}

    def add_results(docs):
        for rank, doc in enumerate(docs):
            # content ê¸°ë°˜ í•´ì‹œë¡œ ì¤‘ë³µ ì œê±°
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            score = 1 / (k + rank + 1)
            if content_hash not in scores:
                scores[content_hash] = {"doc": doc, "score": 0}
            scores[content_hash]["score"] += score

    add_results(dense_docs)
    add_results(sparse_docs)

    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    merged = sorted(scores.values(), key=lambda x: x["score"], reverse=True)

    # source ê¸°ì¤€ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì •ë ¬ ìˆœì„œ ìœ ì§€)
    rrf_scores = {}
    for rank, item in enumerate(merged):
        doc = item["doc"]
        source_key = doc.metadata.get("source", f"doc_{rank}")
        rrf_scores[source_key] = item["score"]

    return [item["doc"] for item in merged], rrf_scores


class AnalysisService:
    """ë¶„ì„ ì„œë¹„ìŠ¤"""

    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.embedding_manager = EmbeddingManager()
        self.embeddings = self.embedding_manager.get_embeddings()

        self.db_manager = VectorStoreManager(self.embeddings)
        try:
            self.db_manager.load_vectorstore()
        except Exception as e:
            raise RuntimeError(f"ë²¡í„° DBë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

        self.llm = get_llm()
        self.analysis_prompt = self._load_prompt("analysis_ko.prt")
        self.make_query_prompt = self._load_prompt("make_query_ko.prt")
        self.logger = ChainLogger()

        # BM25ìš© ë¬¸ì„œ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ 1íšŒ)
        self.bm25_retriever = None

    def _load_prompt(self, name) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        project_root = Path(os.environ.get('PROJECT_ROOT', Path.cwd()))
        prompt_path = project_root / f"app/core/prompt/{name}"
        return prompt_path.read_text(encoding="utf-8")

    def analyze(self, request: AnalysisRequest) -> AnalysisResponse:
        """
        ë¶„ì„ ì‹¤í–‰

        Args:
            request: AnalysisRequest

        Returns:
            AnalysisResponse
        """
        try:
            # 1. ì´ë¯¸ì§€ íŒŒì¼ì„ Cloudinaryì— ì¸ì¦ ì—…ë¡œë“œ
            logger.info("ğŸ“¤ ì´ë¯¸ì§€ë¥¼ Cloudinaryì— ì—…ë¡œë“œ ì¤‘...")
            image_data = request.image_file.file.read()
            upload_result = cloudinary.upload_authenticated_image(
                image_data=image_data,
                expire_minutes=Config.CLOUDINARY_EXPIRE_MINUTES
            )
            image_url = upload_result["secure_url"]
            logger.info(f"âœ… ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ: {image_url}")

            # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + BM25 + RRF)
            logger.info("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘...")

            # Dense ê²€ìƒ‰
            dense_docs_with_scores = self.db_manager.vectorstore.similarity_search_with_score(
                request.user_state,
                k=Config.TOP_K
            )
            dense_docs = [doc for doc, score in dense_docs_with_scores]
            dense_scores = {doc.metadata.get("source", str(i)): score for i, (doc, score) in enumerate(dense_docs_with_scores)}
            logger.info(f"   âœ“ Dense: {len(dense_docs)}ê°œ ë¬¸ì„œ")

            # BM25 ê²€ìƒ‰
            if self.bm25_retriever is None:
                try:
                    # Chromaì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    all_results = self.db_manager.vectorstore.get()
                    if all_results and all_results.get("documents"):
                        # ë¬¸ìì—´ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                        from langchain_core.documents import Document
                        all_docs = [
                            Document(page_content=doc, metadata=meta)
                            for doc, meta in zip(all_results["documents"], all_results["metadatas"])
                        ]
                        self.bm25_retriever = self.db_manager.get_bm25_retriever(all_docs)
                    else:
                        logger.warning("âš ï¸  BM25 ë¬¸ì„œ ì—†ìŒ, Denseë§Œ ì‚¬ìš©")
                        self.bm25_retriever = None
                except Exception as e:
                    logger.warning(f"âš ï¸  BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì‹¤íŒ¨: {e}, Denseë§Œ ì‚¬ìš©")
                    self.bm25_retriever = None

            sparse_docs = []
            sparse_scores = {}
            if self.bm25_retriever:
                sparse_docs = self.bm25_retriever.invoke(request.user_state)
                logger.info(f"   âœ“ BM25: {len(sparse_docs)}ê°œ ë¬¸ì„œ")

            # RRF ë³‘í•©
            rrf_scores = {}
            if sparse_docs:
                search_results, rrf_scores = _merge_with_rrf(dense_docs, sparse_docs, k=Config.RRF_K)
                logger.info(f"   âœ“ RRF ë³‘í•©: {len(search_results)}ê°œ ë¬¸ì„œ")
            else:
                search_results = dense_docs
                logger.info(f"   âœ“ Denseë§Œ ì‚¬ìš©")
                rrf_scores = dense_scores

            # ìƒìœ„ 7ê°œë¡œ ì œí•œ
            search_results = search_results[:Config.TOP_K]

            # 3. ì²´ì¸ êµ¬ì„±
            retriever = self.db_manager.get_retriever()
            chain, query_generator = build_analysis_chain(
                retriever=retriever,
                llm=self.llm,
                analysis_prompt=self.analysis_prompt,
                make_query_prompt=self.make_query_prompt,
                user_state=request.user_state,
                image_url=image_url,
            )

            # 4. ì²´ì¸ ì‹¤í–‰ (ì´ë¯¸ ê²€ìƒ‰í•œ ë¬¸ì„œë“¤ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • ê°€ëŠ¥)
            # í˜„ì¬ëŠ” ê¸°ì¡´ retriever ì‚¬ìš©, ì¶”í›„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
            raw_response = chain.invoke(request.user_state)

            # 5. JSON ì¶”ì¶œ
            analysis = extract_json(raw_response)

            # 6. LLM ì›ë³¸ ì‘ë‹µ ì¶”ì¶œ (ì´ë¯¸ì§€ ë¶„ì„ ë° ì¿¼ë¦¬ ìƒì„± ê²°ê³¼)
            llm_raw_response = query_generator.raw_response if query_generator.raw_response else {}

            # 7. ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ (source ëª©ë¡)
            references = [doc.metadata.get("source", f"doc_{i}") for i, doc in enumerate(search_results)]

            # 8. ë¡œê¹…ìš© ì •ë³´ ì¤€ë¹„ (Dense, BM25, RRF ì ìˆ˜)
            search_metadata = []
            for i, doc in enumerate(search_results):
                source_key = doc.metadata.get("source", f"doc_{i}")
                search_metadata.append({
                    "rank": i + 1,
                    "source": source_key,
                    "dense_score": dense_scores.get(source_key, None),
                    "bm25_score": sparse_scores.get(source_key, None),
                    "rrf_score": rrf_scores.get(source_key, None),
                })

            # 7. ë¡œê·¸ ì €ì¥
            log_path = self.logger.save_analysis(
                image_url=image_url,
                user_state=request.user_state,
                search_results=search_results,
                analysis=analysis,
                image_detail=Config.IMAGE_DETAIL,
                model=Config.LLM_MODEL,
                search_metadata=search_metadata,
                llm_raw_response=llm_raw_response,
            )

            return AnalysisResponse(
                status="success",
                analysis=analysis,
                references=references
            )

        except Exception as e:
            logger.exception(f"âŒ ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ")
            return AnalysisResponse(
                status="error",
                analysis={},
                error=str(e),
            )


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_service = None


def get_analysis_service() -> AnalysisService:
    """ë¶„ì„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _service
    if _service is None:
        _service = AnalysisService()
    return _service
