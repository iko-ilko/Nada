"""
ë¡œê¹… ëª¨ë“ˆ
ë¶„ì„ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
from src.config import Config


class AnalysisLogger:
    """ë¶„ì„ ê²°ê³¼ ë¡œê¹…"""

    def __init__(self, log_dir: str = None):
        """
        AnalysisLogger ì´ˆê¸°í™”

        Args:
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: PROJECT_ROOT/logs)
        """
        if log_dir is None:
            project_root = Path(os.environ.get('PROJECT_ROOT', Path.cwd()))
            log_dir = str(project_root / "logs")

        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

    def save_analysis(
        self,
        image_url: str,
        user_state: str,
        search_results: List[Any],
        analysis: Dict[str, Any],
        image_detail: str,
        model: str = None  # Configì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜µì…˜
    ) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"analysis_{timestamp}.json"
        log_filepath = self.log_dir / log_filename

        # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        papers_info = self._extract_papers_info(search_results)

        # ë¡œê·¸ ë°ì´í„° êµ¬ì„±
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "config": {
                    "LLM_MODEL": Config.LLM_MODEL,
                    "EMBEDDING_MODEL": Config.EMBEDDING_MODEL,
                    "IMAGE_DETAIL": image_detail,
                    "CHUNK_SIZE": Config.CHUNK_SIZE,
                    "CHUNK_OVERLAP": Config.CHUNK_OVERLAP,
                    "TOP_K": Config.TOP_K,
                },
                "image": {
                    "url": image_url,
                    "detail_level": image_detail,
                },
                "input": {
                    "user_state": user_state,
                },
                "search": {
                    "total_results": len(search_results),
                    "papers": papers_info,
                }
            },
            "analysis": analysis,
        }

        # JSONìœ¼ë¡œ ì €ì¥
        try:
            with open(log_filepath, "w", encoding="utf-8") as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ’¾ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_filepath}")
            return str(log_filepath)

        except Exception as e:
            print(f"âŒ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _extract_papers_info(self, search_results: List[Any]) -> List[Dict[str, str]]:
        """
        ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            search_results: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

        Returns:
            List: ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        """
        papers_info = []

        for i, doc in enumerate(search_results, 1):
            paper_info = {
                "rank": i,
                "source": doc.metadata.get("source", "Unknown"),
                "type": doc.metadata.get("type", "Unknown"),
                "page": doc.metadata.get("page", "Unknown"),
                "content_preview": doc.page_content[:300] if hasattr(doc, 'page_content') else "",
                # RAG ê²€ì¦ìš©: ì‹¤ì œ ì „ë‹¬ëœ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
                "full_content": doc.page_content if hasattr(doc, 'page_content') else "",
            }
            papers_info.append(paper_info)

        return papers_info
