# ğŸš€ RAG ì‹œìŠ¤í…œ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (2ì‹œê°„ ì‹¤ì „)

**ëª©í‘œ**: ê¸°ë³¸ RAG ì‹œìŠ¤í…œì„ 2ì‹œê°„ ë‚´ì— êµ¬í˜„í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ê¸°

---

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„

### í•„ìˆ˜
- Python 3.9 ì´ìƒ
- í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ 2-3ê°œ
- í…ìŠ¤íŠ¸ ì—ë””í„° ë˜ëŠ” IDE

### ì„ íƒ (ë¹„ìš© ì ˆê°)
- OpenAI API í‚¤ (ì„ íƒ) - ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

---

## âš¡ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### Step 1: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```bash
mkdir rag_system
cd rag_system
mkdir -p data/papers
mkdir src
```

### Step 2: ì˜ì¡´ì„± ì„¤ì¹˜
```bash
# ìµœì†Œí•œì˜ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜ (ë¹ ë¥¸ ì‹œì‘)
pip install langchain langchain-community chromadb sentence-transformers pypdf python-dotenv

# LLM ì‚¬ìš© (ì„ íƒ)
pip install langchain-openai  # OpenAI ì‚¬ìš©ì‹œ
# ë˜ëŠ” ë¡œì»¬ LLM ì‚¬ìš© (ë‹¤ìŒ ì„¹ì…˜ ì°¸ì¡°)
```

### Step 3: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
- `data/papers/` ì— PDF íŒŒì¼ 2-3ê°œ ë³µì‚¬

### Step 4: ìµœì†Œ êµ¬í˜„ ì½”ë“œ
```python
# main.py
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# 1. ë¬¸ì„œ ë¡œë“œ
loader = PyPDFLoader("data/papers/your_paper.pdf")
docs = loader.load()

# 2. ì²­í‚¹
splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50
)
chunks = splitter.split_documents(docs)

# 3. ì„ë² ë”© + ë²¡í„° DB
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(
    chunks,
    embeddings,
    persist_directory="./chroma_db"
)

# 4. RAG ì²´ì¸
llm = ChatOpenAI(model_name="gpt-3.5-turbo")
chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 5. ì§ˆì˜
result = chain({"query": "ë…¼ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì€?"})
print(result["result"])
```

---

## ğŸ¯ ìƒì„¸ êµ¬í˜„ (ëª¨ë“ˆí™”)

### í”„ë¡œì íŠ¸ êµ¬ì¡° (ì™„ì „)
```
rag_system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ db.py
â”‚   â””â”€â”€ rag.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ papers/
â”‚       â”œâ”€â”€ paper1.pdf
â”‚       â”œâ”€â”€ paper2.pdf
â”‚       â””â”€â”€ paper3.pdf
â”œâ”€â”€ chroma_db/
â”‚   â””â”€â”€ (Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ main.py
```

### 1ï¸âƒ£ config.py - ì„¤ì • íŒŒì¼
```python
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # LLM ì„¤ì •
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    LLM_MODEL = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
    LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.3"))

    # ì„ë² ë”© ì„¤ì •
    EMBEDDING_MODEL = os.getenv(
        "EMBEDDING_MODEL",
        "all-MiniLM-L6-v2"  # ë¹ ë¥¸ ì‹œì‘ìš©
    )

    # ë²¡í„° DB ì„¤ì •
    CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "./chroma_db")
    COLLECTION_NAME = "papers"

    # ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "300"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "50"))

    # RAG ì„¤ì •
    TOP_K = int(os.getenv("TOP_K", "3"))

    # ê²½ë¡œ ì„¤ì •
    PAPERS_DIR = os.getenv("PAPERS_DIR", "./data/papers")

    @classmethod
    def validate(cls):
        """ì„¤ì • ê²€ì¦"""
        if not os.path.exists(cls.PAPERS_DIR):
            os.makedirs(cls.PAPERS_DIR, exist_ok=True)
            print(f"ğŸ“ {cls.PAPERS_DIR} ë””ë ‰í† ë¦¬ ìƒì„±ë¨")
```

### 2ï¸âƒ£ loader.py - ë¬¸ì„œ ë¡œë”
```python
import os
from pathlib import Path
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.schema import Document
from typing import List
from config import Config

class DocumentLoader:
    """ë¬¸ì„œ ë¡œë”© ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€"""

    def __init__(self, papers_dir: str = None):
        self.papers_dir = papers_dir or Config.PAPERS_DIR

    def load_single_pdf(self, filepath: str) -> List[Document]:
        """ë‹¨ì¼ PDF ë¡œë“œ"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")

        loader = PyPDFLoader(filepath)
        docs = loader.load()

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        filename = os.path.basename(filepath)
        for doc in docs:
            doc.metadata.update({
                "source": filename,
                "filepath": filepath,
                "type": "pdf"
            })

        print(f"âœ… {filename} ë¡œë“œ ì™„ë£Œ: {len(docs)}í˜ì´ì§€")
        return docs

    def load_all_pdfs(self) -> List[Document]:
        """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDF ë¡œë“œ"""
        if not os.path.exists(self.papers_dir):
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.papers_dir}")

        pdf_files = list(Path(self.papers_dir).glob("*.pdf"))

        if not pdf_files:
            print(f"âš ï¸  {self.papers_dir} ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []

        all_docs = []
        for pdf_file in pdf_files:
            try:
                docs = self.load_single_pdf(str(pdf_file))
                all_docs.extend(docs)
            except Exception as e:
                print(f"âŒ {pdf_file} ë¡œë“œ ì‹¤íŒ¨: {e}")

        total_pages = sum(1 for doc in all_docs)
        print(f"\nğŸ“Š ì´ {len(pdf_files)}ê°œ íŒŒì¼, {total_pages}í˜ì´ì§€ ë¡œë“œë¨")
        return all_docs
```

### 3ï¸âƒ£ embedder.py - ì„ë² ë”© ê´€ë¦¬
```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from config import Config

class EmbeddingManager:
    """ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬"""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.EMBEDDING_MODEL
        self.embeddings = None

    def get_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self.embeddings is not None:
            return self.embeddings

        if self.model_name.startswith("text-embedding"):
            # OpenAI ì„ë² ë”©
            print(f"ğŸ”„ OpenAI ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.model_name}")
            self.embeddings = OpenAIEmbeddings(
                model=self.model_name,
                api_key=Config.OPENAI_API_KEY
            )
        else:
            # ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”©
            print(f"ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.model_name}")
            print("   (ì²« ì‹¤í–‰ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: 300MB ì •ë„)")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.model_name
            )

        return self.embeddings

# ì„ë² ë”© ëª¨ë¸ ì˜µì…˜
EMBEDDING_OPTIONS = {
    "light": "all-MiniLM-L6-v2",  # ê°€ë³ê³  ë¹ ë¦„ â­ ì¶”ì²œ
    "medium": "all-mpnet-base-v2",  # ì¤‘ê°„ í¬ê¸°
    "large": "all-MiniLM-L12-v2",  # ì¢€ ë” ì •í™•í•¨
    "korean": "ko-e5-base",  # í•œêµ­ì–´ íŠ¹í™” (ì„¤ì¹˜ í•„ìš”)
}
```

### 4ï¸âƒ£ db.py - ë²¡í„° DB ê´€ë¦¬
```python
import os
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import Config
from typing import List
from langchain.schema import Document

class VectorDB:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""

    def __init__(self, embeddings, collection_name: str = None):
        self.embeddings = embeddings
        self.collection_name = collection_name or Config.COLLECTION_NAME
        self.persist_dir = Config.CHROMA_DB_PATH
        self.vectorstore = None

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ì²­í‚¹"""
        print(f"\nğŸ”ª ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
        print(f"   ì²­í¬ í¬ê¸°: {Config.CHUNK_SIZE} í† í°")
        print(f"   ì˜¤ë²„ë©: {Config.CHUNK_OVERLAP} í† í°")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )

        chunks = splitter.split_documents(documents)
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")
        return chunks

    def create_vectorstore(self, chunks: List[Document]):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        print(f"\nğŸ”„ ë²¡í„° ì„ë² ë”© ì¤‘...")
        print(f"   ëª¨ë¸: {Config.EMBEDDING_MODEL}")
        print(f"   ì €ì¥ ìœ„ì¹˜: {self.persist_dir}")

        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_dir,
            collection_name=self.collection_name
        )

        self.vectorstore.persist()
        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")
        return self.vectorstore

    def load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        if not os.path.exists(self.persist_dir):
            raise FileNotFoundError(f"ë²¡í„° DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.persist_dir}")

        print(f"ğŸ“‚ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘: {self.persist_dir}")
        self.vectorstore = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings,
            collection_name=self.collection_name
        )
        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        return self.vectorstore

    def get_retriever(self):
        """Retriever ë°˜í™˜"""
        if self.vectorstore is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": Config.TOP_K}
        )
```

### 5ï¸âƒ£ rag.py - RAG ì²´ì¸
```python
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from config import Config
from typing import Dict, Any

class RAGSystem:
    """RAG ì‹œìŠ¤í…œ"""

    def __init__(self, retriever):
        self.retriever = retriever
        self.llm = None
        self.chain = None
        self._init_llm()
        self._init_chain()

    def _init_llm(self):
        """LLM ì´ˆê¸°í™”"""
        print(f"\nğŸ”„ LLM ì´ˆê¸°í™” ì¤‘: {Config.LLM_MODEL}")

        if Config.OPENAI_API_KEY:
            self.llm = ChatOpenAI(
                model_name=Config.LLM_MODEL,
                temperature=Config.LLM_TEMPERATURE,
                api_key=Config.OPENAI_API_KEY
            )
            print(f"âœ… OpenAI LLM ì¤€ë¹„ë¨")
        else:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (Ollama í•„ìš”)
            try:
                from langchain.llms import Ollama
                self.llm = Ollama(model="mistral")
                print(f"âœ… ë¡œì»¬ Ollama LLM ì¤€ë¹„ë¨")
                print(f"   âš ï¸  Ollama ì„¤ì¹˜ í•„ìš”: https://ollama.ai")
            except Exception as e:
                print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"   OPENAI_API_KEYë¥¼ .envì— ì„¤ì •í•˜ê±°ë‚˜ Ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”")
                raise

    def _init_chain(self):
        """RAG ì²´ì¸ ì´ˆê¸°í™”"""
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = """ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì •ë³´ê°€ ì—†ìœ¼ë©´ "ë¬¸ì„œì— ì´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. í•­ìƒ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        self.chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ "stuff" ì‚¬ìš©
            retriever=self.retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )

        print(f"âœ… RAG ì²´ì¸ ì¤€ë¹„ë¨")

    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        if not self.chain:
            raise ValueError("RAG ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print(f"ğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘...")

        result = self.chain({"query": question})

        answer = result["result"]
        sources = result["source_documents"]

        print(f"\nâœ… ë‹µë³€ ì™„ë£Œ\n")
        print(f"ğŸ“ ë‹µë³€:\n{answer}")

        if sources:
            print(f"\nğŸ“š ì¶œì²˜ ë¬¸ì„œ:")
            for i, doc in enumerate(sources, 1):
                source = doc.metadata.get("source", "Unknown")
                page = doc.metadata.get("page", "Unknown")
                print(f"   {i}. {source} (í˜ì´ì§€ {page})")

        return {
            "answer": answer,
            "sources": sources,
            "source_metadata": [doc.metadata for doc in sources]
        }
```

### 6ï¸âƒ£ main.py - ë©”ì¸ ì‹¤í–‰ íŒŒì¼
```python
import os
from config import Config
from loader import DocumentLoader
from embedder import EmbeddingManager
from db import VectorDB
from rag import RAGSystem

def main():
    print("=" * 60)
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)

    # ì„¤ì • ê²€ì¦
    Config.validate()

    # 1ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ
    print("\n" + "=" * 60)
    print("1ï¸âƒ£  STEP 1: ë¬¸ì„œ ë¡œë“œ")
    print("=" * 60)

    loader = DocumentLoader(Config.PAPERS_DIR)
    documents = loader.load_all_pdfs()

    if not documents:
        print("âŒ ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ {Config.PAPERS_DIR} ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”")
        return

    # 2ï¸âƒ£ ì„ë² ë”© ì´ˆê¸°í™”
    print("\n" + "=" * 60)
    print("2ï¸âƒ£  STEP 2: ì„ë² ë”© ì´ˆê¸°í™”")
    print("=" * 60)

    embedding_manager = EmbeddingManager()
    embeddings = embedding_manager.get_embeddings()

    # 3ï¸âƒ£ ë²¡í„° DB ìƒì„± ë˜ëŠ” ë¡œë“œ
    print("\n" + "=" * 60)
    print("3ï¸âƒ£  STEP 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
    print("=" * 60)

    vector_db = VectorDB(embeddings)

    if os.path.exists(Config.CHROMA_DB_PATH):
        print("ğŸ“‚ ê¸°ì¡´ ë²¡í„° DB ë°œê²¬")
        vector_db.load_vectorstore()
    else:
        print("âœ¨ ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„± ì¤‘...")
        chunks = vector_db.split_documents(documents)
        vector_db.create_vectorstore(chunks)

    # 4ï¸âƒ£ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n" + "=" * 60)
    print("4ï¸âƒ£  STEP 4: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    print("=" * 60)

    retriever = vector_db.get_retriever()
    rag_system = RAGSystem(retriever)

    # 5ï¸âƒ£ ëŒ€í™”í˜• ì§ˆì˜
    print("\n" + "=" * 60)
    print("5ï¸âƒ£  STEP 5: ëŒ€í™”í˜• ì§ˆì˜")
    print("=" * 60)
    print("\nğŸ’¡ íŒ: 'quit' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì—¬ ì¢…ë£Œí•˜ì„¸ìš”\n")

    while True:
        try:
            question = input("ğŸ”¹ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

            if question.lower() in ["quit", "exit", "q"]:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ!")
                break

            if not question:
                print("âš ï¸  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”\n")
                continue

            result = rag_system.query(question)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ!")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n")

if __name__ == "__main__":
    main()
```

### 7ï¸âƒ£ .env íŒŒì¼
```bash
# OpenAI API (ì„ íƒì‚¬í•­)
OPENAI_API_KEY=sk-...

# LLM ëª¨ë¸
LLM_MODEL=gpt-3.5-turbo
LLM_TEMPERATURE=0.3

# ì„ë² ë”© ëª¨ë¸
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ë²¡í„° DB
CHROMA_DB_PATH=./chroma_db
CHUNK_SIZE=300
CHUNK_OVERLAP=50
TOP_K=3

# ë°ì´í„° ê²½ë¡œ
PAPERS_DIR=./data/papers
```

### 8ï¸âƒ£ requirements.txt
```txt
langchain==0.1.14
langchain-community==0.0.29
langchain-openai==0.1.9
chromadb==0.5.0
sentence-transformers==2.7.0
pypdf==4.0.1
python-dotenv==1.0.0
pydantic==2.7.0
```

---

## ğŸ¬ ì‹¤í–‰ ë°©ë²•

### ì²« ë²ˆì§¸ ì‹¤í–‰
```bash
# 1. í”„ë¡œì íŠ¸ ìƒì„±
mkdir rag_system && cd rag_system

# 2. íŒŒì¼ êµ¬ì¡° ìƒì„± (ìœ„ì˜ ì½”ë“œ ì°¸ì¡°)
# src/, data/papers/, ê° py íŒŒì¼ ìƒì„±

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. .env íŒŒì¼ ì‘ì„± (OpenAI í‚¤ ì¶”ê°€ ë˜ëŠ” ë¡œì»¬ LLM ì„¤ì •)

# 5. PDF íŒŒì¼ ì¶”ê°€
# data/papers/ ì— PDF íŒŒì¼ ë³µì‚¬

# 6. ì‹¤í–‰
python main.py
```

### ì‹¤í–‰ ì˜ˆì‹œ
```
==============================================================
ğŸš€ RAG ì‹œìŠ¤í…œ ì‹œì‘
==============================================================

==============================================================
1ï¸âƒ£  STEP 1: ë¬¸ì„œ ë¡œë“œ
==============================================================

ğŸ“ ./data/papers ë””ë ‰í† ë¦¬ ìƒì„±ë¨
âœ… paper1.pdf ë¡œë“œ ì™„ë£Œ: 15í˜ì´ì§€
âœ… paper2.pdf ë¡œë“œ ì™„ë£Œ: 12í˜ì´ì§€

ğŸ“Š ì´ 2ê°œ íŒŒì¼, 27í˜ì´ì§€ ë¡œë“œë¨

==============================================================
2ï¸âƒ£  STEP 2: ì„ë² ë”© ì´ˆê¸°í™”
==============================================================

ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: all-MiniLM-L6-v2
âœ… ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ë¨

==============================================================
3ï¸âƒ£  STEP 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
==============================================================

âœ¨ ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„± ì¤‘...

ğŸ”ª ë¬¸ì„œ ì²­í‚¹ ì¤‘...
   ì²­í¬ í¬ê¸°: 300 í† í°
   ì˜¤ë²„ë©: 50 í† í°
âœ… ì²­í‚¹ ì™„ë£Œ: 89ê°œ ì²­í¬ ìƒì„±ë¨

ğŸ”„ ë²¡í„° ì„ë² ë”© ì¤‘...
   ëª¨ë¸: all-MiniLM-L6-v2
   ì €ì¥ ìœ„ì¹˜: ./chroma_db
âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ

==============================================================
4ï¸âƒ£  STEP 4: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
==============================================================

ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘: gpt-3.5-turbo
âœ… OpenAI LLM ì¤€ë¹„ë¨
âœ… RAG ì²´ì¸ ì¤€ë¹„ë¨

==============================================================
5ï¸âƒ£  STEP 5: ëŒ€í™”í˜• ì§ˆì˜
==============================================================

ğŸ’¡ íŒ: 'quit' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì—¬ ì¢…ë£Œí•˜ì„¸ìš”

ğŸ”¹ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë¬´ì—‡ì¸ê°€?

â“ ì§ˆë¬¸: ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë¬´ì—‡ì¸ê°€?
ğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘...

âœ… ë‹µë³€ ì™„ë£Œ

ğŸ“ ë‹µë³€:
ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ...

ğŸ“š ì¶œì²˜ ë¬¸ì„œ:
   1. paper1.pdf (í˜ì´ì§€ 3)
   2. paper1.pdf (í˜ì´ì§€ 5)
```

---

## ğŸ› ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ: "No module named 'langchain'"
```bash
pip install --upgrade langchain langchain-community langchain-openai
```

### ë¬¸ì œ: ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
```python
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
EMBEDDING_MODEL=all-MiniLM-L6-v2  # 33MB
# ëŒ€ì‹ 
EMBEDDING_MODEL=all-mpnet-base-v2  # 438MB (ëŠë¦¼)
```

### ë¬¸ì œ: OpenAI API í‚¤ ì˜¤ë¥˜
```bash
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY í™•ì¸
# ë˜ëŠ” ë¡œì»¬ LLM ì‚¬ìš© (Ollama)
# https://ollama.ai ì—ì„œ ì„¤ì¹˜
```

### ë¬¸ì œ: ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° ë˜ëŠ” ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
EMBEDDING_MODEL=all-MiniLM-L6-v2  # ê²½ëŸ‰
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì°¸ê³ )

### ì„ë² ë”© ì†ë„ (100ê°œ ì²­í¬ ê¸°ì¤€)
- `all-MiniLM-L6-v2`: ~2ì´ˆ (ê¶Œì¥)
- `all-mpnet-base-v2`: ~8ì´ˆ
- OpenAI API: ~3ì´ˆ (ë„¤íŠ¸ì›Œí¬ ì˜ì¡´)

### ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„
- ë²¡í„° ê²€ìƒ‰: ~50ms
- LLM ë‹µë³€ ìƒì„±: 1-10ì´ˆ (ëª¨ë¸/ê¸¸ì´ ì˜ì¡´)
- ì „ì²´: ~1-15ì´ˆ

### ë©”ëª¨ë¦¬ ì‚¬ìš©
- `all-MiniLM-L6-v2`: ~300MB
- Chroma DB: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„
- ì „ì²´ ì‹œìŠ¤í…œ: ~500MB-1GB

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
2. **ê³ ê¸‰ ê²€ìƒ‰**: MMR (Maximal Marginal Relevance) ê²€ìƒ‰ ì ìš©
3. **ë©€í‹°í„´ ëŒ€í™”**: ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•œ ì§ˆì˜ì‘ë‹µ
4. **ì›¹ ì¸í„°í˜ì´ìŠ¤**: FastAPI + Reactë¡œ ì›¹ ì•± êµ¬ì¶•
5. **í”„ë¡œë•ì…˜ ë°°í¬**: Docker + í´ë¼ìš°ë“œ ë°°í¬

---

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€**
