"""
ë¶„ì„ í”Œë¡œìš° ë¡œê¹… ëª¨ë“ˆ
ë¶„ì„ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""
import json
import logging
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
from app.core.config import Config

logger = logging.getLogger(__name__)


class ChainLogger:
    """ë¶„ì„ í”Œë¡œìš° ë¡œê¹…"""

    def __init__(self, log_dir: str = None):
        """ChainLogger ì´ˆê¸°í™”"""
        if log_dir is None:
            log_dir = Config.LOGS_DIR

        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

    def save_analysis(
        self,
        image_url: str,
        user_state: str,
        search_results: List[Any],
        analysis: Dict[str, Any],
        image_detail: str,
        model: str = None,
        search_scores: List[float] = None
    ) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"analysis_{timestamp}.json"
        log_filepath = self.log_dir / log_filename

        papers_info = self._extract_papers_info(search_results, search_scores)

        log_data = {
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "config": {
                    "LLM_MODEL": Config.LLM_MODEL,
                    "EMBEDDING_MODEL": Config.EMBEDDING_MODEL,
                    "IMAGE_DETAIL": image_detail,
                    "CHUNK_SIZE": Config.CHUNK_SIZE,
                    "CHUNK_OVERLAP": Config.CHUNK_OVERLAP,
                    "TOP_K": Config.TOP_K,
                },
                "image": {
                    "url": image_url,
                    "detail_level": image_detail,
                },
                "input": {
                    "user_state": user_state,
                },
                "search": {
                    "total_results": len(search_results),
                    "papers": papers_info,
                }
            },
            "analysis": analysis,
        }

        try:
            with open(log_filepath, "w", encoding="utf-8") as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ’¾ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_filepath}")
            return str(log_filepath)

        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _extract_papers_info(self, search_results: List[Any], search_scores: List[float] = None) -> List[Dict[str, str]]:
        """ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        papers_info = []

        for i, doc in enumerate(search_results, 1):
            paper_info = {
                "rank": i,
                "source": doc.metadata.get("source", "Unknown"),
                "type": doc.metadata.get("type", "Unknown"),
                "page": doc.metadata.get("page", "Unknown"),
                "similarity_score": search_scores[i - 1] if search_scores and i - 1 < len(search_scores) else None,
                "content_preview": doc.page_content[:300] if hasattr(doc, 'page_content') else "",
                "full_content": doc.page_content if hasattr(doc, 'page_content') else "",
            }
            papers_info.append(paper_info)

        return papers_info
